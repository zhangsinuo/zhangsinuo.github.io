<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://zhangsinuo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zhangsinuo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-17T14:48:42+01:00</updated><id>https://zhangsinuo.github.io/feed.xml</id><title type="html">ICLR Blogposts 2024</title><subtitle>Home to the 2024 ICLR Blogposts track </subtitle><entry><title type="html">One-Hour Survey of Large Language Models —— Quick Mastery of New Techniques, Concepts, and Case Studies</title><link href="https://zhangsinuo.github.io/blog/dictionary-llm/" rel="alternate" type="text/html" title="One-Hour Survey of Large Language Models —— Quick Mastery of New Techniques, Concepts, and Case Studies"/><published>2024-05-07T00:00:00+02:00</published><updated>2024-05-07T00:00:00+02:00</updated><id>https://zhangsinuo.github.io/blog/dictionary-llm</id><content type="html" xml:base="https://zhangsinuo.github.io/blog/dictionary-llm/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation with billions of parameters that require significant computational resources and vast data for training. These artificial neural networks, mainly transformers, are pre-trained using self-supervised and semi-supervised learning techniques. The boom in the field of LLM has attracted many researchers and practitioners to devote themselves to related research. Notable examples include OpenAI’s GPT models (e.g., GPT-3.5 and GPT-4, used in ChatGPT), Google’s PaLM (used in Bard), and Meta’s LLaMa, as well as BLOOM, Ernie 3.0 Titan, and Anthropic’s Claude 2, thus promoting the advancement of technology in this field.</p> <p>As technology advances rapidly, many technical terms have emerged, some of which may not be easily understood. For example, ‘in-context learning’ refers to the model’s ability to understand and respond based on the provided context, while ‘instruction tuning’ involves refining the model to respond to specific instructions more effectively. This proliferation of terms can make it challenging for researchers and users of LLMs to stay abreast of the latest advancements, creating unfavorable conditions for developing LLM technology. Therefore, sorting out and summarizing these proprietary terms has become an urgent demand for the LLM technology research community. To address this issue, this blog has launched “Dictionary LLM”, which aims to organize and explain the existing terminologies of LLMs and provides a convenient reference for related researchers so that they can quickly understand the basic concepts of LLMs and then deeply catch the technology related to large models. This blog and the ‘Dictionary LLM’ are particularly useful for researchers, practitioners, and anyone interested in the field of LLMs, offering a comprehensive resource for both newcomers and experienced professionals.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-dictionary-llm/over-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-dictionary-llm/over-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-dictionary-llm/over-1400.webp"/> <img src="/assets/img/2024-05-07-dictionary-llm/over.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> A summary of Dictionary of LLM. </div> <p>In the following parts, Section 2 will explore pre-training technologies, detailing the training datasets used, the architectural nuances of various models, and the principles of model alignment with human values and expectations. Section 3 will delve into the nuances of fine-tuning methods, focusing on the art and science of prompt engineering and its impact on model performance. Finally, Section 4 will discuss the varied applications and challenges in LLM research, addressing critical issues like model-generated hallucinations and their implications.</p> <h2 id="pre-training-technologies">Pre-training Technologies</h2> <p>Pre-training is the first stage in the training process of machine learning models. It involves training the model on a large dataset to learn general features or representations of the data. These general features often include linguistic structures, syntax, common phrases, and contextual relationships, which form the foundational understanding of the model. Once pre-training is complete, fine-tuning follows, where the model is further trained on a smaller, task-specific dataset to specialize it for particular tasks. The pre-training phase helps the model leverage knowledge from a vast amount of data, making it more proficient in handling specific tasks with less data during the fine-tuning phase. This large dataset usually comprises diverse text sources, ranging from books and articles to websites, encompassing a wide array of topics and styles. This approach is commonly used in training large language models and deep learning models. Here, we briefly introduce three typical pre-training architectures for LLMs and an important technology, alignment.</p> <h3 id="encoder-decoder-architecture">Encoder-Decoder Architecture</h3> <p><strong>Definition:</strong> The Encoder-Decoder Architecture is a fundamental concept in machine translation and sequence-to-sequence tasks. It is composed of two parts:</p> <ol> <li> <p>Encoder: This component processes the input sequence and encodes it into a context vector of fixed size, capturing all the relevant information of the sequence.</p> </li> <li> <p>Decoder: This component takes the context vector generated by the encoder and produces an output sequence step-by-step. It often translates a sequence into another language or generates responses in a dialogue system.</p> </li> </ol> <p>This architecture is useful in handling sequences of varying lengths and is crucial in many Natural Language Processing (NLP) and time-series prediction tasks. So far, only a few LLMs are built based on the encoder-decoder architecture, e.g., <d-cite key="vaswani2017attention"></d-cite>.</p> <p><strong>Case Studies:</strong> Take English to French Translation as an example.</p> <ol> <li> <p>Encoder: Processes an English sentence (“The weather is nice today”) by converting words into vectors and generating a context vector encapsulating the sentence’s meaning.</p> </li> <li> <p>Decoder: Uses the context vector to sequentially generate a translated French sentence (“Le temps est agréable aujourd’hui”), considering the context and previously generated words.</p> </li> <li> <p>Key Feature: Attention mechanism, allowing the decoder to focus on relevant parts of the input for each translated word.</p> </li> <li> <p>Outcome: The architecture enables effective translation by handling varying lengths and complexities in language sequences.</p> </li> </ol> <p><strong>Related Work:</strong> More information can be seen in Vaswani et al.<d-cite key="vaswani2017attention"></d-cite>.</p> <h3 id="causal-decoder-architecture">Causal Decoder Architecture</h3> <p><strong>Definition:</strong> The causal decoder architecture incorporates the unidirectional attention mask to guarantee that each input token can only attend to the past tokens and itself. The input and output tokens are processed similarly through the decoder. This feature is particularly useful for language modeling, where each word’s prediction depends solely on the previous words. This architecture is beneficial in maintaining the causal integrity of sequences, making it an ideal choice for tasks that require understanding or generating sequences in a causal or chronological order. As representative language models of this architecture, the GPT-series models are developed based on the causal-decoder architecture.</p> <p><strong>Case Studies:</strong> Take text generation with the GPT Series as an example.</p> <ol> <li> <p>Architecture: The GPT models use a causal-decoder structure where each token in the sequence can only attend to previous tokens and itself. This is achieved through a unidirectional attention mask.</p> </li> <li> <p>Process: When generating text, the model receives a prompt (e.g., “The sun sets over the”). It processes each token in the prompt, predicting the next word based on the previous context. For instance, after “the”, it might predict “horizon”.</p> </li> <li> <p>Application: This architecture is ideal for tasks like story generation, where the narrative must follow a logical and chronological sequence.</p> </li> <li> <p>Outcome: GPT models, thanks to the causal-decoder architecture, can generate coherent and contextually relevant text that follows a logical temporal order, making them powerful tools for various language generation tasks.</p> </li> </ol> <p><strong>Related Work:</strong> More information can be seen in Brown et al.<d-cite key="brown2020language"></d-cite>.</p> <h3 id="prefix-decoder-architecture">Prefix Decoder Architecture</h3> <p><strong>Definition:</strong> The Prefix Decoder Architecture is a specific design used in language model architectures <d-cite key="dong2019unified"></d-cite>. It is also known as the non-causal decoder, which revises the masking mechanism of causal decoders. This enables it to perform bidirectional attention over the prefix tokens while only using unidirectional attention on generated tokens. This allows prefix decoders to bidirectionally encode the prefix sequence and autoregressively predict the output tokens individually. During encoding and decoding, the same parameters are shared. This makes the prefix decoder similar to the encoder-decoder architecture.</p> <p><strong>Case Studies:</strong> Unified Language Model Using Prefix Decoder.</p> <ol> <li> <p>Architecture: This design allows the model to bidirectionally process a given prefix (pre-existing sequence of tokens) while generating future tokens in an autoregressive (one-by-one) manner.</p> </li> <li> <p>Process: Suppose the model is given a prefix like “In a world where technology”. It first encodes this prefix bidirectionally, understanding the context from both sides. Then, for generating the subsequent text, it switches to a unidirectional approach, predicting one token at a time based on the prefix and previously generated tokens.</p> </li> <li> <p>Application: This architecture is beneficial in tasks like text completion or question answering, where understanding the full context of the prefix is crucial for generating relevant and coherent continuations.</p> </li> <li> <p>Outcome: Models using the Prefix Decoder architecture effectively leverage both bidirectional and unidirectional attention mechanisms, making them versatile for a range of language understanding and generation tasks. They combine aspects of encoder-decoder architectures with the autoregressive generation capabilities of causal decoders.</p> </li> </ol> <p><strong>Related Work:</strong> More information can be seen in Zhang et al.<d-cite key="zhang2022examining"></d-cite>.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-dictionary-llm/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-dictionary-llm/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-dictionary-llm/7-1400.webp"/> <img src="/assets/img/2024-05-07-dictionary-llm/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> A comparison of the attention patterns in three mainstream architectures. Here, the yellow, green, blue, and grey rectangles indicate the encoder-encoder attention, encoder-decoder attention, decoder-decoder attention, and masked attention, respectively. </div> <h3 id="alignment">Alignment</h3> <p><strong>Definition:</strong> Alignment is a crucial aspect of pre-training and involves ensuring that the model’s outputs are not only accurate but also ethically aligned with human values, reducing the risks of biases and harmful outputs. Ethical alignment involves addressing issues like fairness, privacy, transparency, and accountability. It ensures that models do not perpetuate biases or stereotypes and respect user privacy and data security. Alignment tax (or safety tax) refers to the extra costs involved in aligning an AI system with human ethics and morality as opposed to building an unaligned system. These costs may arise from extensive data curation to eliminate biases, the development of complex algorithms that can discern and adhere to ethical norms, and ongoing monitoring and evaluation to ensure continued alignment. Alignment is key to building trust in AI systems, as it ensures that users can anticipate how a model might behave in different scenarios and that the model’s reasoning and outputs are interpretable and justifiable.</p> <p><strong>Case Studies:</strong> Implementing Ethical Standards in an AI Recruitment Tool</p> <ol> <li> <p>Objective: To develop an AI recruitment tool that not only assesses candidates effectively but also aligns with ethical standards, ensuring fairness and diversity.</p> </li> <li>Challenges and Costs: <ul> <li>Data Curation: Extensive effort to curate a diverse and unbiased dataset representing a wide range of demographics.</li> <li>Algorithm Development: Creating complex algorithms capable of making fair, unbiased decisions and adhering to ethical norms.</li> <li>Ongoing Monitoring: Continuous evaluation to maintain alignment, involving regular updates and checks.</li> </ul> </li> <li>Performance Trade-offs: <ul> <li>Processing Time: Slower decision-making due to the complexity of ethical algorithms.</li> <li>Accuracy: Potentially lower accuracy in specific contexts to avoid ethical risks, like discriminating against certain groups.</li> </ul> </li> <li>Outcome: The AI recruitment tool successfully facilitates fair and diverse hiring practices. Despite slower processing and potential accuracy trade-offs, the tool gains trust and acceptance due to its ethical and fair decision-making process.</li> </ol> <p><strong>Related Works:</strong> More information can be seen in Christiano et al.<d-cite key="christiano2017deep"></d-cite>. and in Askell et al.<d-cite key="askell2021general"></d-cite>.</p> <h2 id="fine-tuning-methods">Fine-tuning Methods</h2> <p>Fine-tuning Large Language Models (LLMs) is customizing these models to perform better for specific tasks or within certain domains. Fine-tuning is a process where a pre-trained LLM is trained on a smaller dataset specific to the task or domain of interest. Unlike pre-training, which involves learning from a broad range of data, fine-tuning focuses on specific patterns, terminology, and nuances relevant to the task, making the model more adept in those particular areas. This helps slightly adjust the model’s parameters to improve its performance on the given task. Fine-tuning is essential for tailoring general-purpose LLMs to specific industry needs, cultural contexts, or user groups, thus maximizing their practical utility and effectiveness. Those fine-tuning methods can mainly divided into four classes: Reinforcement learning from human feedback (RLHF), instruction tuning, parameter-efficient model adaption, and retrieval augment generation.</p> <h3 id="supervised-fine-tuning">Supervised Fine-tuning</h3> <p><strong>Definition:</strong> Supervised fine-tuning is a process that involves further training a pre-trained model using a smaller and domain-specific dataset that has labeled examples. This process aims to guide the model to adjust its parameters based on the labeled examples and perform better on specific tasks like classification or regression. This process uses the model’s general knowledge acquired during pre-training and sharpens its abilities to perform a specific task based on the labeled examples provided in the fine-tuning dataset. Supervised fine-tuning is a widespread practice that helps to adapt large pre-trained models to particular domains or tasks, improving their performance and making them more applicable to the problem. It is highly correlated with RLHF and is typically considered the first step in an RLHF procedure.</p> <p><strong>Case Study:</strong> Fine-Tuning a Language Model for Medical Diagnosis Assistance</p> <ol> <li> <p>Objective: To adapt a general language model for use in medical diagnosis, enabling it to understand and respond accurately to medical queries.</p> </li> <li>Process: <ul> <li>Pre-Trained Model: Start with a language model pre-trained on a vast corpus of general text.</li> <li>Fine-Tuning Dataset: Utilize a dataset of medical conversations and diagnoses, accurately labeled by medical professionals.</li> <li>Training: The model is fine-tuned with this medical dataset, learning to adjust its responses to fit the medical context accurately.</li> </ul> </li> <li> <p>Application: The fine-tuned model is employed in a medical chatbot, assisting doctors by providing preliminary diagnoses based on patient symptoms described in natural language.</p> </li> <li>Outcome: Post fine-tuning, the model shows significantly improved accuracy in understanding and responding to medical queries, making it a valuable tool for healthcare professionals.</li> </ol> <p><strong>Related Works:</strong> More information can be seen in Ouyang et al.<d-cite key="ouyang2022training"></d-cite>.</p> <h3 id="reinforcement-learning-from-human-feedback">Reinforcement Learning from Human Feedback</h3> <p><strong>Definition:</strong> RLHF, short for Reinforcement Learning from Human Feedback, is a training method that uses human feedback to train machine learning models. Unlike traditional reinforcement learning methods that rely solely on reward signals, RLHF incorporates human evaluations into the learning process to align the models with human values and preferences. This approach can be particularly useful in training language models to generate human-like text and understand natural language. By refining the model’s responses based on human feedback, RLHF can improve its performance and align it with human values. RLAIF, short for Reinforcement Learning from AI feedback <d-cite key="bai2022constitutional"></d-cite>is an improvement over RLHF, as it merges traditional reinforcement learning methodologies with feedback generated from other AI models, usually an aligned LLM, instead of relying solely on human feedback. This evolved learning approach has shown promising results and opens up new avenues for research in the field.</p> <p><strong>Case Study:</strong> Training a Customer Service Chatbot using RLHF</p> <ol> <li> <p>Objective: To train a chatbot for effective and empathetic customer service interactions.</p> </li> <li>Process: <ul> <li>Initial Model: Begin with a pre-trained language model capable of basic conversational responses.</li> <li>Human Feedback: Introduce human feedback by having language experts review and rate the chatbot’s responses in simulated customer service scenarios.</li> <li>Training Loop: Use the feedback to adjust the model’s parameters, encouraging it to generate responses that align more closely with the desired quality of customer service communication.</li> </ul> </li> <li> <p>Application: Deploy the chatbot in a real-world customer service environment, handling inquiries and providing assistance.</p> </li> <li>Outcome: The chatbot, trained with RLHF, shows improved ability to handle customer queries with appropriate, empathetic, and effective responses, leading to higher customer satisfaction.</li> </ol> <p><strong>Related Works:</strong> More information can be seen in Christiano et al.<d-cite key="christiano2017deep"></d-cite>.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-dictionary-llm/RLHF-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-dictionary-llm/RLHF-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-dictionary-llm/RLHF-1400.webp"/> <img src="/assets/img/2024-05-07-dictionary-llm/RLHF.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> <d-footnote>Figure from Ouyang et al.<d-cite key="ouyang2022training"></d-cite></d-footnote>Performing the RLHF algorithm involves three steps. Firstly, fine-tune a pre-trained LLM through supervised learning. Secondly, generate multiple responses for each query and rank them based on human preference. Finally, optimize the language model through RL by maximizing the feedback from the reward model. </div> <h3 id="instruction-tuning">Instruction Tuning</h3> <p><strong>Defnition:</strong> Instruction tuning is an approach to fine-tune pre-trained LLMs using a collection of formatted instances (called instruction) in natural language. It is similar to supervised fine-tuning and multi-task prompted training. To perform instruction tuning, we must first collect or create instruction-formatted instances. These instances are then used to fine-tune LLMs in a supervised learning manner, for example, by training with the sequence-to-sequence loss. After instruction tuning, LLMs can demonstrate superior abilities to generalize to unseen tasks, even in multilingual settings. Instruction tuning is extensively researched and is a common feature in existing language models such as InstructGPT and GPT-4.</p> <p><strong>Case Study:</strong> Enhancing a Language Model for Multilingual Task Handling</p> <ol> <li> <p>Objective: To improve a language model’s capability in handling a variety of tasks across multiple languages.</p> </li> <li>Process: <ul> <li>Collection of Instructions: Gather a diverse set of instruction-formatted instances in various languages, covering different tasks such as translation, summarization, and question-answering.</li> <li>Fine-Tuning: Use these instances to fine-tune the LLM in a supervised manner, employing a sequence-to-sequence loss function to guide the learning process.</li> </ul> </li> <li> <p>Application: The fine-tuned model is employed in a multilingual virtual assistant capable of understanding and executing a wide range of user instructions in different languages.</p> </li> <li>Outcome: Post instruction tuning, the LLM shows enhanced performance in accurately interpreting and executing diverse tasks in multiple languages, demonstrating improved generalization abilities.</li> </ol> <p><strong>Related Works:</strong> More information can be seen in Lou et al.<d-cite key="lou2023prompt"></d-cite>.</p> <h3 id="adapter-tuning">Adapter Tuning</h3> <p><strong>Defnition:</strong> Adapter tuning incorporates small neural network modules (called adapter) into the Transformer models. A bottleneck architecture is used to implement the adapter module. The architecture compresses the original feature vector into a smaller dimension, followed by a nonlinear transformation, and then recovers it to the original dimension. The adapter modules are integrated into each Transformer layer, usually inserted serially after each of a Transformer layer’s two core parts (i.e., attention layer and feed-forward layer). This technique is an alternative to fine-tuning and involves updating only the parameters of the adapter modules while learning on a downstream task. It allows for a lighter-weight adaptation of the pre-trained model to the new task.</p> <p><strong>Case Study:</strong> Enhancing a Language Model for Multilingual Task Handling</p> <ol> <li> <p>Objective: To adapt a pre-trained Transformer-based language model for a specific language pair translation task, say English to Japanese.</p> </li> <li>Process: <ul> <li>Adapter Integration: Small adapter modules are inserted into each Transformer layer of the pre-trained model. These adapters have a bottleneck architecture, compressing and then expanding the feature vectors.</li> <li>Training: Only the adapter modules are trained on a dataset of English-Japanese sentence pairs. The rest of the model’s parameters remain frozen.</li> </ul> </li> <li> <p>Application: The adapted model is used in a translation service to provide accurate and contextually relevant English-to-Japanese translations.</p> </li> <li>Outcome: The model, with its newly tuned adapters, demonstrates improved translation accuracy between English and Japanese, achieving this with significantly less computational resource expenditure compared to full model fine-tuning.</li> </ol> <p><strong>Related Works:</strong> More information can be seen in Houlsby et al.<d-cite key="houlsby2019parameter"></d-cite>.</p> <h3 id="prefix-tuning">Prefix Tuning</h3> <p><strong>Definition:</strong> Prefix tuning is a technique used to improve the performance of language models. It involves adding a set of trainable continuous vectors called “prefixes” to each Transformer layer. These prefixes are specific to the task and can be considered virtual token embedding. A reparameterization trick is used to optimize the prefixes. This involves training a Multi-Layer Perceptron (MLP) function that maps a smaller matrix to the parameter matrix of prefixes instead of directly optimizing them. This trick is effective in ensuring stable training. Once the optimization is complete, the mapping function is discarded, and only the derived prefix vectors are kept to enhance task-specific performance. Since only the prefix parameters are trained, this can lead to a more efficient model optimization.</p> <p><strong>Case Study:</strong> Improving Sentiment Analysis with Prefix Tuning</p> <ol> <li> <p>Objective: To refine a pre-trained language model for more accurate sentiment analysis on product reviews.</p> </li> <li>Process: <ul> <li>Adding Prefixes: Trainable prefix vectors are added to each layer of the Transformer model. These prefixes act like virtual token embeddings tailored to the sentiment analysis task.</li> <li>Training Method: A Multi-Layer Perceptron (MLP) function is used to optimize these prefixes, mapping a smaller matrix to the parameter matrix of prefixes for stable training.</li> </ul> </li> <li> <p>Application: The model, with optimized prefixes, is deployed to analyze customer reviews on an e-commerce platform, categorizing them into positive, negative, or neutral sentiments.</p> </li> <li>Outcome: Post prefix tuning, the model shows enhanced accuracy in detecting sentiments from text, efficiently identifying customer opinions with minimal additional computational resources.</li> </ol> <p><strong>Related Works:</strong> More information can be seen in Li et al.<d-cite key="li2021prefix"></d-cite>.</p> <h3 id="prompt-tuning">Prompt Tuning</h3> <p><strong>Definition:</strong> Different from prefix tuning, prompt tuning mainly focuses on incorporating trainable prompt vectors at the input layer. The input prompt is modified to direct the model to generate the desired outputs while keeping the model parameters unchanged. The input text is augmented by including a group of soft prompt tokens based on discrete prompting methods to achieve this. This prompt-augmented input is then used to solve specific downstream tasks. To implement this technique, task-specific prompt embedding is combined with the input text embedding fed into language models. P-tuning <d-cite key="liu2023gpt"></d-cite> has proposed a free form to combine the context, prompt, and target tokens, which can be applied to the architectures for natural language understanding and generation.</p> <p><strong>Case Study:</strong> Enhancing a Chatbot for Customer Service using Prompt Tuning</p> <ol> <li> <p>Objective: To optimize a pre-trained language model for a specific task in customer service, such as handling common inquiries or complaints.</p> </li> <li>Process: <ul> <li>Input Modification: Incorporate a set of trainable soft prompt tokens into the input text. These prompts are designed to steer the model towards generating responses suitable for customer service scenarios.</li> <li>Prompt Embedding: Combine task-specific prompt embeddings with the input text embeddings before feeding them into the model.</li> </ul> </li> <li> <p>Application: The adjusted model is used in a customer service chatbot, which interacts with customers, addressing their queries and concerns more effectively and contextually.</p> </li> <li>Outcome: After prompt tuning, the chatbot demonstrates improved performance in understanding and responding to customer service-related queries, providing more relevant and helpful responses without the need for extensive retraining of the entire model.</li> </ol> <p><strong>Related Works:</strong> More information can be seen in Lester et al.<d-cite key="lester2021power"></d-cite>.</p> <h3 id="low-rank-adaptation">Low-Rank Adaptation</h3> <p><strong>Definition:</strong> Low-Rank Adaptation (LoRA) is a technique used to mitigate the computational complexity associated with neural networks, especially when fine-tuning large language models on devices with limited resources. LoRA imposes a low-rank constraint to approximate the updated matrix at each dense layer, thereby reducing the trainable parameters for adapting to downstream tasks. The main advantage of LoRA is that it can significantly save memory and storage usage (e.g., VRAM). Additionally, it allows for keeping only a single large model copy while maintaining several task-specific low-rank decomposition matrices for adapting to different downstream tasks.</p> <p><strong>Case Study:</strong> Implementing LoRA for a Multitask Language Model</p> <ol> <li> <p>Objective: To adapt a large language model efficiently for multiple downstream tasks (like text classification, summarization, and translation) on a device with limited computational resources.</p> </li> <li>Process: <ul> <li>Low-Rank Constraint: Apply LoRA by introducing low-rank matrices to approximate updates in the model’s dense layers, significantly reducing the number of trainable parameters.</li> <li>Task-Specific Adaptation: Maintain a single copy of the large model while creating several low-rank decomposition matrices, each tailored to a specific task.</li> </ul> </li> <li> <p>Application: The adapted model is used in an environment where it needs to switch between different NLP tasks based on user input, such as a versatile chatbot or a multi-functional text processing tool.</p> </li> <li>Outcome: With LoRA, the model efficiently handles multiple tasks without the need for separate model copies for each task, conserving memory and computational resources while maintaining high performance across various NLP tasks.</li> </ol> <p><strong>Related Works:</strong> More information can be seen in Hu et al.<d-cite key="hu2021lora"></d-cite>.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-dictionary-llm/3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-dictionary-llm/3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-dictionary-llm/3-1400.webp"/> <img src="/assets/img/2024-05-07-dictionary-llm/3.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> An illustration of four different parameter-efficient fine-tuning methods. </div> <h2 id="prompt-engineering">Prompt Engineering</h2> <p>After pre-training or adaptation tuning, a major approach to using LLMs is to design suitable prompting strategies for solving various tasks and show their special abilities, i.e., emergent abilities. Prompt tuning involves creating effective prompts that guide the model to generate desired responses. This process, known as prompt engineering <d-cite key="zheng2023judging"></d-cite>, requires a careful design of prompts to elicit accurate and relevant responses from the model. A well-designed prompt is essential in eliciting the abilities of language models to accomplish specific tasks. Typically, four key ingredients depict the functionality of a prompt for eliciting the abilities of language models to complete tasks. These include task description, input data, contextual information, and prompt style. In-context learning is a common prompting method that involves formulating the task description and/or demonstrations in natural language text. Additionally, chain-of-thought prompting can enhance in-context learning by involving a series of intermediate reasoning steps in prompts.</p> <h3 id="emergent-abilities">Emergent Abilities</h3> <p><strong>Definition:</strong> Emergent abilities are unique capabilities that are only present in large language models (LLMs) and not in smaller ones. This is one of the most prominent features differentiating LLMs from previous pre-trained language models (PLMs). When the scale reaches a certain level, emergent abilities occur as LLMs perform significantly above random. In principle, we are more concerned with emergent abilities that can be applied to solve various tasks. For instance, in-context learning, instruction following, and step-by-step reasoning are three abilities for LLMs and representative models that possess these abilities. We will introduce these abilities in the following sections.</p> <p><strong>Case Study:</strong> See in In-context Learning and Step-by-step reasoning.</p> <p><strong>Related Works:</strong> More information can be seen in Wei et al.<d-cite key="wei2022emergent"></d-cite>.</p> <h3 id="scaling-law">Scaling Law</h3> <p><strong>Definition:</strong> In the field of statistics, a scaling law refers to a functional relationship between two quantities where a relative change in one quantity results in a proportional change in the other quantity, independent of their initial size. In the context of LLMs (Language Model Models), a neural scaling law is a type of scaling law that relates the parameters of a family of neural networks. A neural model can be characterized by four parameters: model size, training dataset size, training cost, and performance after training. Each of these four variables can be precisely defined as a real number, and they are empirically found to be related by simple statistical laws known as “scaling laws.” These laws are usually expressed as N, D, C, and L, where N represents the number of parameters, D represents the dataset size, C represents the computing cost, and L represents the loss. For instance, KM scaling law <d-cite key="kaplan2020scaling"></d-cite> and Chinchilla scaling law <d-cite key="hoffmann2022training"></d-cite>.</p> <p><strong>Case Study:</strong> Applying the Kaplan-Meier (KM) Scaling Law to Optimize an LLM</p> <ol> <li> <p>Objective: To use the KM scaling law to determine the optimal balance between model size, dataset size, and computational cost for a language model designed for natural language understanding.</p> </li> <li>Scaling Law - KM Scaling Law: <ul> <li>Description: This law suggests that increasing the number of parameters (N) and the dataset size (D) leads to a decrease in the loss (L), but with diminishing returns.</li> <li>Application: Deciding on the model size and training dataset size to achieve a desired level of performance within a feasible computational budget.</li> </ul> </li> <li>Process: <ul> <li>Model Parameters (N): Select an initial number of parameters based on computational resources.</li> <li>Dataset Size (D): Choose a dataset size that complements the chosen model size, adhering to the scaling law.</li> <li>Training and Loss (L): Train the model and observe the loss, adjusting N and D as necessary to optimize performance.</li> </ul> </li> <li>Outcome: By following the KM scaling law, the development team efficiently scales the model to a size where it achieves high performance in natural language understanding tasks, balancing the trade-off between computational cost and accuracy. This case demonstrates how scaling laws can guide the efficient and effective development of LLMs.</li> </ol> <p><strong>Related Works:</strong> More information can be seen in Kaplan et al.<d-cite key="kaplan2020scaling"></d-cite> and Hoffman et al. <d-cite key="hoffmann2022training"></d-cite>.</p> <h3 id="in-context-learning">In-context Learning</h3> <p><strong>Definition:</strong> The concept of in-context learning (ICL) was introduced by GPT-3. Essentially, ICL allows a language model to generate the expected output for test instances by completing the word sequence of input text without requiring additional training or gradient updates. This is particularly useful in scenarios where models must adapt to new information or tasks based on the context provided. The language model is provided with natural language instruction and/or several task demonstrations, which allows it to complete the word sequence of input text and generate the expected output for the test instances.</p> <p><strong>Case Study:</strong> Using GPT-3 for Real-Time Language Translation via In-Context Learning</p> <ol> <li> <p>Objective: To demonstrate GPT-3’s in-context learning ability by using it for real-time translation between English and Spanish.</p> </li> <li>In-Context Learning Process: <ul> <li>Input: Provide GPT-3 with a set of example translations between English and Spanish within the prompt.</li> <li>Task Demonstration: Include a few examples of sentences in English followed by their Spanish translations.In-Context Learning: GPT-3 uses these examples to understand the task-translating English text into Spanish.</li> </ul> </li> <li> <p>Application: Implement GPT-3 in a translation tool where users input English sentences, and the model provides real-time Spanish translations.</p> </li> <li>Outcome: Leveraging in-context learning, GPT-3 successfully translates new English sentences into Spanish accurately, demonstrating its ability to quickly adapt to the translation task with just a few contextual examples, without any specific training for translation.</li> </ol> <p><strong>Related Works:</strong> More information can be seen in Brown et al.<d-cite key="brown2020language"></d-cite>.</p> <h3 id="context-window">Context Window</h3> <p><strong>Definition:</strong> A context window is a specific range of words or tokens surrounding a particular word within a text, i.e., the length of the context that LLMs can utilize. It is used to understand the linguistic context of that word and analyze the relationships and dependencies between words. However, Transformer-based LMs have a limitation of a limited context length due to the quadratic computational costs in both time and memory. Despite this, there is a growing need for long context windows in applications such as PDF processing and story writing.</p> <p><strong>Case Study:</strong> Enhancing Story Writing with Extended Context Window</p> <ol> <li> <p>Objective: To improve a Transformer-based language model’s ability to write coherent and consistent stories by extending its context window.</p> </li> <li>Challenge: <ul> <li>Limited Context Window: The standard Transformer model struggles with longer narratives due to its limited context window, potentially losing track of earlier plot points or character details.</li> <li>Need for Extension: Story writing requires keeping track of a long narrative, characters, and plot developments, necessitating a longer context window.</li> </ul> </li> <li>Solution: <ul> <li>Implementation: Employ techniques like memory-augmented neural networks or sparse attention mechanisms to extend the context window the model can consider.</li> <li>Application: The model is used to generate stories, where it now maintains coherence over longer narratives by referring back to events and characters introduced earlier in the text.</li> </ul> </li> <li>Outcome: With an extended context window, the model produces more coherent and contextually consistent stories, successfully recalling and integrating earlier plot elements and character interactions throughout the narrative.</li> </ol> <p><strong>Related Works:</strong> More information can be seen in Yuan et al.<d-cite key="yuan2022wordcraft"></d-cite>.</p> <h3 id="step-by-step-reasoning">Step-by-step reasoning</h3> <p><strong>Definition:</strong> Step-by-step reasoning refers to a methodical process of arriving at conclusions or solving problems by following a logical sequence of steps or stages. These sequences often involve breaking down a complex problem into smaller, more manageable parts, analyzing each part individually, and then synthesizing the insights to form a coherent solution. This process is grounded in logical, systematic progression from one point to the next based on rules, facts, or rational analysis. In this process, rules serve as guidelines or principles that direct the reasoning path, while rational analysis involves critically evaluating information, assumptions, and implications at each step. Step-by-step reasoning enhances clarity, reduces the likelihood of errors, and facilitates easier identification and correction of mistakes, making it a highly effective approach to systematic problem-solving. While highly effective, this method can be time-consuming and may not always be suitable for problems requiring creative or out-of-the-box thinking.</p> <p><strong>Case Study:</strong> Using Step-by-Step Reasoning in AI for Medical Diagnosis</p> <ol> <li> <p>Objective: To implement an AI system using step-by-step reasoning for accurate and systematic medical diagnosis.</p> </li> <li>Process: <ul> <li>Problem Decomposition: The AI system breaks down a patient’s symptoms into individual factors (e.g., intensity, duration, related conditions).</li> <li>Sequential Analysis: Each factor is analyzed in a logical sequence, referencing medical rules and data.</li> <li>Synthesis of Insights: The system then synthesizes these insights to form a preliminary diagnosis, considering the interrelations of symptoms and medical guidelines.</li> </ul> </li> <li> <p>Application: The AI is used in a healthcare setting, assisting doctors by providing preliminary diagnoses based on patient-reported symptoms.</p> </li> <li>Outcome: The AI system, through step-by-step reasoning, offers accurate, logically derived diagnoses, enhancing clarity and reducing errors. This systematic approach improves diagnostic efficiency but may be time-consuming compared to more heuristic methods.</li> </ol> <p><strong>Related Works:</strong> More information can be seen in Wei et al.<d-cite key="wei2022chain"></d-cite>.</p> <h3 id="chain-of-thought-prompt">Chain-of-thought Prompt</h3> <p><strong>Definition:</strong> Chain-of-thought prompt is a type of prompt or instruction that encourages a person or a machine to follow a sequential or logical progression of ideas (i.e. step-by-step reasoning), akin to how a chain links together. This concept can be seen in various areas, such as creative writing, brainstorming, problem-solving, and machine learning. The core idea behind a chain-of-thought prompt is to provide a structured pathway for exploring or generating ideas, whether in human thought processes or machine-generated outputs. This structured approach can lead to a more coherent, logical, and meaningful exploration or generation of ideas, solutions, or narratives. Typically, CoT can be used with ICL in two major settings: the few-shot <d-cite key="li2022advance"></d-cite> and zero-shot settings <d-cite key="kojima2022large"></d-cite>.</p> <p><strong>Case Study:</strong> Using Chain-of-Thought Prompting in AI for Complex Math Problem Solving</p> <ol> <li> <p>Objective: To enhance an AI model’s capability to solve complex mathematical problems using chain-of-thought prompts.</p> </li> <li>Process: <ul> <li>CoT Prompt Design: Develop prompts that guide the AI to break down a complex math problem into smaller, sequential steps.</li> <li>Sequential Reasoning: The AI follows the prompt to tackle each part of the problem methodically, documenting its reasoning process at each step.</li> <li>Solution Synthesis: The AI synthesizes the insights from each step to arrive at the final answer.</li> </ul> </li> <li> <p>Application: Implement AI in an educational tool to assist students in understanding and solving complex math problems.</p> </li> <li>Outcome: The AI, guided by CoT prompts, demonstrates an improved ability to methodically solve complex math problems, providing step-by-step explanations that enhance understanding and learning for students. This approach leads to more coherent and logically structured problem-solving compared to direct answer generation.</li> </ol> <p><strong>Related Works:</strong> More information can be seen in Wei et al.<d-cite key="wei2022chain"></d-cite>, Li et al.<d-cite key="li2022advance"></d-cite>, and Kojima et al.<d-cite key="kojima2022large"></d-cite>.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[The rapid advancements in large language models (LLMs) have given rise to numerous technical terms, which can be overwhelming for researchers. Therefore, there is an urgent need to sort and summarize these terms for the LLM technology research community. To cater to this need, we proposed a blog named &quot;Dictionary LLM&quot;, which organizes and explains the existing terminologies of LLMs, providing a convenient reference for researchers. This dictionary helps them quickly understand the basic concepts of LLMs and delve deeper into the technology related to large models. The &quot;Dictionary LLM&quot; is essential for keeping up with the rapid developments in this dynamic field, ensuring that researchers and practitioners have access to the latest terminology and concepts.]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://zhangsinuo.github.io/blog/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2024-05-07T00:00:00+02:00</published><updated>2024-05-07T00:00:00+02:00</updated><id>https://zhangsinuo.github.io/blog/distill-example</id><content type="html" xml:base="https://zhangsinuo.github.io/blog/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.html path="assets/img/2024-05-07-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/iclr-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2024-05-07-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/9-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/7-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/8-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/10-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/11-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/12-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/7-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %} 
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2024-05-07-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span> <span class="na">src=</span><span class="s">"{{ 'assets/html/2024-05-07-distill-example/plotly_demo_1.html' | relative_url }}"</span> <span class="na">frameborder=</span><span class="s">'0'</span> <span class="na">scrolling=</span><span class="s">'no'</span> <span class="na">height=</span><span class="s">"600px"</span> <span class="na">width=</span><span class="s">"100%"</span><span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/assets/html/2024-05-07-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p> <p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p> <p><strong>Note:</strong> This is not supported for local rendering!</p> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div> <div class="jekyll-diagrams diagrams mermaid"> <svg id="mermaid-1702820927058" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1702820927058 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1702820927058 .node circle,#mermaid-1702820927058 .node ellipse,#mermaid-1702820927058 .node polygon,#mermaid-1702820927058 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1702820927058 .node.clickable{cursor:pointer}#mermaid-1702820927058 .arrowheadPath{fill:#333}#mermaid-1702820927058 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1702820927058 .edgeLabel{background-color:#e8e8e8}#mermaid-1702820927058 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1702820927058 .cluster text{fill:#333}#mermaid-1702820927058 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1702820927058 .actor{stroke:#ccf;fill:#ececff}#mermaid-1702820927058 text.actor{fill:#000;stroke:none}#mermaid-1702820927058 .actor-line{stroke:grey}#mermaid-1702820927058 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1702820927058 .messageLine0,#mermaid-1702820927058 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1702820927058 #arrowhead{fill:#333}#mermaid-1702820927058 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1702820927058 .messageText{fill:#333;stroke:none}#mermaid-1702820927058 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1702820927058 .labelText,#mermaid-1702820927058 .loopText{fill:#000;stroke:none}#mermaid-1702820927058 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1702820927058 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1702820927058 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1702820927058 .section{stroke:none;opacity:.2}#mermaid-1702820927058 .section0{fill:rgba(102,102,255,.49)}#mermaid-1702820927058 .section2{fill:#fff400}#mermaid-1702820927058 .section1,#mermaid-1702820927058 .section3{fill:#fff;opacity:.2}#mermaid-1702820927058 .sectionTitle0,#mermaid-1702820927058 .sectionTitle1,#mermaid-1702820927058 .sectionTitle2,#mermaid-1702820927058 .sectionTitle3{fill:#333}#mermaid-1702820927058 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1702820927058 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1702820927058 .grid path{stroke-width:0}#mermaid-1702820927058 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1702820927058 .task{stroke-width:2}#mermaid-1702820927058 .taskText{text-anchor:middle;font-size:11px}#mermaid-1702820927058 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1702820927058 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1702820927058 .taskText0,#mermaid-1702820927058 .taskText1,#mermaid-1702820927058 .taskText2,#mermaid-1702820927058 .taskText3{fill:#fff}#mermaid-1702820927058 .task0,#mermaid-1702820927058 .task1,#mermaid-1702820927058 .task2,#mermaid-1702820927058 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1702820927058 .taskTextOutside0,#mermaid-1702820927058 .taskTextOutside1,#mermaid-1702820927058 .taskTextOutside2,#mermaid-1702820927058 .taskTextOutside3{fill:#000}#mermaid-1702820927058 .active0,#mermaid-1702820927058 .active1,#mermaid-1702820927058 .active2,#mermaid-1702820927058 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1702820927058 .activeText0,#mermaid-1702820927058 .activeText1,#mermaid-1702820927058 .activeText2,#mermaid-1702820927058 .activeText3{fill:#000!important}#mermaid-1702820927058 .done0,#mermaid-1702820927058 .done1,#mermaid-1702820927058 .done2,#mermaid-1702820927058 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1702820927058 .doneText0,#mermaid-1702820927058 .doneText1,#mermaid-1702820927058 .doneText2,#mermaid-1702820927058 .doneText3{fill:#000!important}#mermaid-1702820927058 .crit0,#mermaid-1702820927058 .crit1,#mermaid-1702820927058 .crit2,#mermaid-1702820927058 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1702820927058 .activeCrit0,#mermaid-1702820927058 .activeCrit1,#mermaid-1702820927058 .activeCrit2,#mermaid-1702820927058 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1702820927058 .doneCrit0,#mermaid-1702820927058 .doneCrit1,#mermaid-1702820927058 .doneCrit2,#mermaid-1702820927058 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1702820927058 .activeCritText0,#mermaid-1702820927058 .activeCritText1,#mermaid-1702820927058 .activeCritText2,#mermaid-1702820927058 .activeCritText3,#mermaid-1702820927058 .doneCritText0,#mermaid-1702820927058 .doneCritText1,#mermaid-1702820927058 .doneCritText2,#mermaid-1702820927058 .doneCritText3{fill:#000!important}#mermaid-1702820927058 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1702820927058 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1702820927058 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1702820927058 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1702820927058 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1702820927058 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1702820927058 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1702820927058 #compositionEnd,#mermaid-1702820927058 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1702820927058 #aggregationEnd,#mermaid-1702820927058 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1702820927058 #dependencyEnd,#mermaid-1702820927058 #dependencyStart,#mermaid-1702820927058 #extensionEnd,#mermaid-1702820927058 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1702820927058 .branch-label,#mermaid-1702820927058 .commit-id,#mermaid-1702820927058 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1702820927058{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item ⋅⋅* Unordered sub-list.</li> <li>Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list</li> <li>And another item.</li> </ol> <p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> <ul> <li>Unordered lists can use asterisks</li> <li>Or minuses</li> <li>Or pluses</li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting. 
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Sample Blog Post (HTML version)</title><link href="https://zhangsinuo.github.io/blog/distill-example2/" rel="alternate" type="text/html" title="Sample Blog Post (HTML version)"/><published>2024-05-07T00:00:00+02:00</published><updated>2024-05-07T00:00:00+02:00</updated><id>https://zhangsinuo.github.io/blog/distill-example2</id><content type="html" xml:base="https://zhangsinuo.github.io/blog/distill-example2/"><![CDATA[<p> This is a sample blog post written in HTML (while the other <a href="/blog/distill-example/">sample post</a> is written in Markdown). Authors have the choice to write in HTML or Markdown. While Markdown is easier to write, HTML gives you more control over the layout of your post. Furthermore, Markdown often interacts in unexpected ways with MathJax and other HTML widgets. If you are having trouble with Markdown, try writing in HTML instead. </p> <p> Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling. </p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code>$$</code>, like <code>$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code>$$</code> and place it as a separate paragraph. Here is an example: $$ \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) $$ </p> <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. You can display images from this repository using the following code:</p> <pre><code>{% include figure.html path="assets/img/2024-05-07-distill-example/iclr.png" class="img-fluid" %}</code></pre> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/iclr-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p> To ensure that there are no namespace conflicts, you must save your asset to your unique directory `/assets/img/2024-05-07-[SUBMISSION NAME]` within your submission. </p> <p> Please avoid using the direct HTML method of embedding images; they may not be properly resized. Some below complex ways to load images (note the different styles of the shapes/shadows): </p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/9-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/7-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/8-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/10-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/11-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/12-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2024-05-07-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2024-05-07-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2024-05-07-distill-example/7-1400.webp"/> <img src="/assets/img/2024-05-07-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3>Interactive Figures</h3> <p> Here's how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work. All that's required is for you to export your figure into HTML format, and make sure that the file exists in the `assets/html/[SUBMISSION NAME]/` directory in this repository's root directory. To embed it into any page, simply insert the following code anywhere into your page. </p> <pre><code>{% include [FIGURE_NAME].html %}</code></pre> <p> For example, the following code can be used to generate the figure underneath it. </p> <pre><code class="language-python">import pandas as pd
import plotly.express as px

df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')

fig = px.density_mapbox(
    df, lat='Latitude', lon='Longitude', z='Magnitude', radius=10,
    center=dict(lat=0, lon=180), zoom=0, mapbox_style="stamen-terrain")
fig.show()

fig.write_html('./assets/html/2024-05-07-distill-example/plotly_demo_1.html')
</code></pre> And then include it with the following: <pre><code class="language-html">&lt;div class="l-page"&gt;
  &lt;iframe src="{{ 'assets/html/2024-05-07-distill-example/plotly_demo_1.html' | relative_url }}" frameborder='0' scrolling='no' height="600px" width="100%"&gt;&lt;/iframe&gt;
&lt;/div&gt;
</code></pre> Voila! <div class="l-page"> <iframe src="/assets/html/2024-05-07-distill-example/plotly_demo_1.html" frameborder='0' scrolling='no' height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p> Citations are then used in the article body with the <code>&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas. </p> <p> The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it. </p> <p> Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well - the authors are human and it's nice for them to have the community associate them with their work. </p> <h2 id="footnotes">Footnotes</h2> <p> Just wrap the text you would like to show up in a footnote in a <code>&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote> </p> <h2 id="code-blocks">Code Blocks</h2> <p> This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag as follows: </p> <pre><code>
{% highlight c++ linenos %}  <br/> code code code <br/> {% endhighlight %}

</code></pre> The keyword `linenos` triggers display of line numbers. You can try toggling it on or off yourself below: <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <h2 id="diagrams">Diagrams</h2> <p> This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="http://mermaid.js.org/">mermaid</a>, <a href="https://plantuml.com/">plantuml</a>, <a href="https://vega.github.io/vega-lite/">vega-lite</a>, etc. </p> <p> <b>Note</b>different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to the <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> README. </p> <p> <b>Note:</b> This is not supported for local rendering! </p> <p> The diagram below was generated by the following code: </p> <pre><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice->>John: Hello John, how are you?
    John-->>Alice: Great!
{% endmermaid %}

</code></pre> <div class='jekyll-diagrams diagrams mermaid'> <svg id="mermaid-1702820927678" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1702820927678 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1702820927678 .node circle,#mermaid-1702820927678 .node ellipse,#mermaid-1702820927678 .node polygon,#mermaid-1702820927678 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1702820927678 .node.clickable{cursor:pointer}#mermaid-1702820927678 .arrowheadPath{fill:#333}#mermaid-1702820927678 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1702820927678 .edgeLabel{background-color:#e8e8e8}#mermaid-1702820927678 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1702820927678 .cluster text{fill:#333}#mermaid-1702820927678 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1702820927678 .actor{stroke:#ccf;fill:#ececff}#mermaid-1702820927678 text.actor{fill:#000;stroke:none}#mermaid-1702820927678 .actor-line{stroke:grey}#mermaid-1702820927678 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1702820927678 .messageLine0,#mermaid-1702820927678 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1702820927678 #arrowhead{fill:#333}#mermaid-1702820927678 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1702820927678 .messageText{fill:#333;stroke:none}#mermaid-1702820927678 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1702820927678 .labelText,#mermaid-1702820927678 .loopText{fill:#000;stroke:none}#mermaid-1702820927678 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1702820927678 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1702820927678 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1702820927678 .section{stroke:none;opacity:.2}#mermaid-1702820927678 .section0{fill:rgba(102,102,255,.49)}#mermaid-1702820927678 .section2{fill:#fff400}#mermaid-1702820927678 .section1,#mermaid-1702820927678 .section3{fill:#fff;opacity:.2}#mermaid-1702820927678 .sectionTitle0,#mermaid-1702820927678 .sectionTitle1,#mermaid-1702820927678 .sectionTitle2,#mermaid-1702820927678 .sectionTitle3{fill:#333}#mermaid-1702820927678 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1702820927678 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1702820927678 .grid path{stroke-width:0}#mermaid-1702820927678 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1702820927678 .task{stroke-width:2}#mermaid-1702820927678 .taskText{text-anchor:middle;font-size:11px}#mermaid-1702820927678 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1702820927678 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1702820927678 .taskText0,#mermaid-1702820927678 .taskText1,#mermaid-1702820927678 .taskText2,#mermaid-1702820927678 .taskText3{fill:#fff}#mermaid-1702820927678 .task0,#mermaid-1702820927678 .task1,#mermaid-1702820927678 .task2,#mermaid-1702820927678 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1702820927678 .taskTextOutside0,#mermaid-1702820927678 .taskTextOutside1,#mermaid-1702820927678 .taskTextOutside2,#mermaid-1702820927678 .taskTextOutside3{fill:#000}#mermaid-1702820927678 .active0,#mermaid-1702820927678 .active1,#mermaid-1702820927678 .active2,#mermaid-1702820927678 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1702820927678 .activeText0,#mermaid-1702820927678 .activeText1,#mermaid-1702820927678 .activeText2,#mermaid-1702820927678 .activeText3{fill:#000!important}#mermaid-1702820927678 .done0,#mermaid-1702820927678 .done1,#mermaid-1702820927678 .done2,#mermaid-1702820927678 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1702820927678 .doneText0,#mermaid-1702820927678 .doneText1,#mermaid-1702820927678 .doneText2,#mermaid-1702820927678 .doneText3{fill:#000!important}#mermaid-1702820927678 .crit0,#mermaid-1702820927678 .crit1,#mermaid-1702820927678 .crit2,#mermaid-1702820927678 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1702820927678 .activeCrit0,#mermaid-1702820927678 .activeCrit1,#mermaid-1702820927678 .activeCrit2,#mermaid-1702820927678 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1702820927678 .doneCrit0,#mermaid-1702820927678 .doneCrit1,#mermaid-1702820927678 .doneCrit2,#mermaid-1702820927678 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1702820927678 .activeCritText0,#mermaid-1702820927678 .activeCritText1,#mermaid-1702820927678 .activeCritText2,#mermaid-1702820927678 .activeCritText3,#mermaid-1702820927678 .doneCritText0,#mermaid-1702820927678 .doneCritText1,#mermaid-1702820927678 .doneCritText2,#mermaid-1702820927678 .doneCritText3{fill:#000!important}#mermaid-1702820927678 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1702820927678 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1702820927678 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1702820927678 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1702820927678 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1702820927678 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1702820927678 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1702820927678 #compositionEnd,#mermaid-1702820927678 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1702820927678 #aggregationEnd,#mermaid-1702820927678 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1702820927678 #dependencyEnd,#mermaid-1702820927678 #dependencyStart,#mermaid-1702820927678 #extensionEnd,#mermaid-1702820927678 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1702820927678 .branch-label,#mermaid-1702820927678 .commit-id,#mermaid-1702820927678 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1702820927678{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <h2 id="tweets">Tweets</h2> <p> An example of displaying a tweet: <div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </p> <p> An example of pulling from a timeline: <div class='jekyll-twitter-plugin'><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </p> <p> For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a> </p> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <h2 id="layouts">Layouts</h2> The main text column is referred to as the body. It's the assumed layout of any direct descendants of the `d-article` element. <div class="fake-img l-body"> <p>.l-body</p> </div> For images you want to display a little larger, try `.l-page`: <div class="fake-img l-page"> <p>.l-page</p> </div> All of these have an outset variant if you want to poke out from the body text a little bit. For instance: <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> Occasionally you'll want to use the full browser width. For this, use `.l-screen`. You can also inset the element a little from the edge of the browser by using the inset variant. <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of `.l-body`-sized text except on mobile screen sizes. <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <h2 id="other-typography">Other Typography?</h2> <p> Emphasis, aka italics, with the <code>&lt;i&gt;&lt;/i&gt;</code> tag <i>emphasis</i>. </p> <p> Strong emphasis, aka bold, with <code>&lt;b&gt;&lt;/b&gt;</code> tag <b>bold</b>. </p> <p> Strikethrough ca be accomplished with the <code>&lt;s&gt;&lt;/s&gt;</code> tag. <s>Scratch this.</s> </p> <ul> <li>First ordered list item</li> <li>Another item</li> <ol> <li>Unordered sub-list. </li> </ol> <li>And another item.</li> </ul> <p> For code, the language can be specified in the class. For example, use <q>language-javascript</q> for Javascript and <q>language-python</q> for Python code. </p> <pre><code class="language-javascript">var s = "JavaScript syntax highlighting";
  alert(s);</code></pre> <pre><code class="language-python">s = "Python syntax highlighting"
  print(s)</code></pre> <pre><code class="language-python">No language indicated, so no syntax highlighting.</code></pre> <p> A table can be created with the <code>&lt;table&gt;</code> element. Below is an example </p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p> <blockquote>Blockquotes can be defined with the &gt;blockquote&lt; tag.</blockquote> </p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry></feed>